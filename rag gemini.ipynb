{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a1b2c3d4",
            "metadata": {},
            "source": [
                "# RAG Q&A with Google Gemini 2.5 Flash\n",
                "\n",
                "This notebook demonstrates a Retrieval-Augmented Generation (RAG) pipeline:\n",
                "1. Load a PDF document\n",
                "2. Split into chunks\n",
                "3. Create embeddings & vector store (FAISS)\n",
                "4. Ask questions using Gemini 2.5 Flash"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b2c3d4e5",
            "metadata": {},
            "source": [
                "## Step 1: Load PDF Document"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "1392a35f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: qdrant-client in .\\rag-env\\Lib\\site-packages (1.17.0)\n",
                        "Requirement already satisfied: langchain in .\\rag-env\\Lib\\site-packages (1.2.10)\n",
                        "Requirement already satisfied: langchain-google-genai in .\\rag-env\\Lib\\site-packages (4.2.0)\n",
                        "Requirement already satisfied: langchain-text-splitters in .\\rag-env\\Lib\\site-packages (1.1.0)\n",
                        "Requirement already satisfied: sentence-transformers in .\\rag-env\\Lib\\site-packages (5.2.2)\n",
                        "Requirement already satisfied: python-dotenv in .\\rag-env\\Lib\\site-packages (1.2.1)\n",
                        "Requirement already satisfied: grpcio>=1.41.0 in .\\rag-env\\Lib\\site-packages (from qdrant-client) (1.76.0)\n",
                        "Requirement already satisfied: httpx>=0.20.0 in .\\rag-env\\Lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
                        "Requirement already satisfied: numpy>=2.3.0 in .\\rag-env\\Lib\\site-packages (from qdrant-client) (2.3.5)\n",
                        "Requirement already satisfied: portalocker<4.0,>=2.7.0 in .\\rag-env\\Lib\\site-packages (from qdrant-client) (3.2.0)\n",
                        "Requirement already satisfied: protobuf>=3.20.0 in .\\rag-env\\Lib\\site-packages (from qdrant-client) (5.29.5)\n",
                        "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in .\\rag-env\\Lib\\site-packages (from qdrant-client) (2.12.5)\n",
                        "Requirement already satisfied: urllib3<3,>=1.26.14 in .\\rag-env\\Lib\\site-packages (from qdrant-client) (2.6.3)\n",
                        "Requirement already satisfied: pywin32>=226 in .\\rag-env\\Lib\\site-packages (from portalocker<4.0,>=2.7.0->qdrant-client) (311)\n",
                        "Requirement already satisfied: langchain-core<2.0.0,>=1.2.10 in .\\rag-env\\Lib\\site-packages (from langchain) (1.2.14)\n",
                        "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in .\\rag-env\\Lib\\site-packages (from langchain) (1.0.8)\n",
                        "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in .\\rag-env\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (1.33)\n",
                        "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in .\\rag-env\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.6.7)\n",
                        "Requirement already satisfied: packaging>=23.2.0 in .\\rag-env\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (24.2)\n",
                        "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in .\\rag-env\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (6.0.3)\n",
                        "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in .\\rag-env\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (9.1.2)\n",
                        "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in .\\rag-env\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (4.15.0)\n",
                        "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in .\\rag-env\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.14.0)\n",
                        "Requirement already satisfied: jsonpointer>=1.9 in .\\rag-env\\Lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain) (3.0.0)\n",
                        "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in .\\rag-env\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.8->langchain) (4.0.0)\n",
                        "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in .\\rag-env\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.8->langchain) (1.0.7)\n",
                        "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in .\\rag-env\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.8->langchain) (0.3.3)\n",
                        "Requirement already satisfied: xxhash>=3.5.0 in .\\rag-env\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.8->langchain) (3.6.0)\n",
                        "Requirement already satisfied: ormsgpack>=1.12.0 in .\\rag-env\\Lib\\site-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain) (1.12.2)\n",
                        "Requirement already satisfied: orjson>=3.10.1 in .\\rag-env\\Lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (3.11.7)\n",
                        "Requirement already satisfied: requests-toolbelt>=1.0.0 in .\\rag-env\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (1.0.0)\n",
                        "Requirement already satisfied: requests>=2.0.0 in .\\rag-env\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (2.32.5)\n",
                        "Requirement already satisfied: zstandard>=0.23.0 in .\\rag-env\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (0.25.0)\n",
                        "Requirement already satisfied: anyio in .\\rag-env\\Lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.12.1)\n",
                        "Requirement already satisfied: certifi in .\\rag-env\\Lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2026.1.4)\n",
                        "Requirement already satisfied: httpcore==1.* in .\\rag-env\\Lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
                        "Requirement already satisfied: idna in .\\rag-env\\Lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.11)\n",
                        "Requirement already satisfied: h11>=0.16 in .\\rag-env\\Lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
                        "Requirement already satisfied: annotated-types>=0.6.0 in .\\rag-env\\Lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
                        "Requirement already satisfied: pydantic-core==2.41.5 in .\\rag-env\\Lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.41.5)\n",
                        "Requirement already satisfied: typing-inspection>=0.4.2 in .\\rag-env\\Lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.2)\n",
                        "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in .\\rag-env\\Lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
                        "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in .\\rag-env\\Lib\\site-packages (from langchain-google-genai) (1.61.0)\n",
                        "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in .\\rag-env\\Lib\\site-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.48.0)\n",
                        "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in .\\rag-env\\Lib\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
                        "Requirement already satisfied: distro<2,>=1.7.0 in .\\rag-env\\Lib\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.9.0)\n",
                        "Requirement already satisfied: sniffio in .\\rag-env\\Lib\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.3.1)\n",
                        "Requirement already satisfied: pyasn1-modules>=0.2.1 in .\\rag-env\\Lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
                        "Requirement already satisfied: cryptography>=38.0.3 in .\\rag-env\\Lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (46.0.4)\n",
                        "Requirement already satisfied: rsa<5,>=3.1.4 in .\\rag-env\\Lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in .\\rag-env\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (3.4.4)\n",
                        "Requirement already satisfied: pyasn1>=0.1.3 in .\\rag-env\\Lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.2)\n",
                        "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in .\\rag-env\\Lib\\site-packages (from sentence-transformers) (5.0.0)\n",
                        "Requirement already satisfied: huggingface-hub>=0.20.0 in .\\rag-env\\Lib\\site-packages (from sentence-transformers) (1.3.7)\n",
                        "Requirement already satisfied: torch>=1.11.0 in .\\rag-env\\Lib\\site-packages (from sentence-transformers) (2.10.0)\n",
                        "Requirement already satisfied: scikit-learn in .\\rag-env\\Lib\\site-packages (from sentence-transformers) (1.8.0)\n",
                        "Requirement already satisfied: scipy in .\\rag-env\\Lib\\site-packages (from sentence-transformers) (1.17.0)\n",
                        "Requirement already satisfied: tqdm in .\\rag-env\\Lib\\site-packages (from sentence-transformers) (4.67.1)\n",
                        "Requirement already satisfied: filelock in .\\rag-env\\Lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.20.3)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in .\\rag-env\\Lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.15)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in .\\rag-env\\Lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
                        "Requirement already satisfied: typer-slim in .\\rag-env\\Lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in .\\rag-env\\Lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in .\\rag-env\\Lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2026.1.0)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in .\\rag-env\\Lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
                        "Requirement already satisfied: shellingham in .\\rag-env\\Lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
                        "Requirement already satisfied: cffi>=2.0.0 in .\\rag-env\\Lib\\site-packages (from cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.0.0)\n",
                        "Requirement already satisfied: pycparser in .\\rag-env\\Lib\\site-packages (from cffi>=2.0.0->cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.0)\n",
                        "Requirement already satisfied: h2<5,>=3 in .\\rag-env\\Lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
                        "Requirement already satisfied: hyperframe<7,>=6.1 in .\\rag-env\\Lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
                        "Requirement already satisfied: hpack<5,>=4.1 in .\\rag-env\\Lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in .\\rag-env\\Lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
                        "Requirement already satisfied: networkx>=2.5.1 in .\\rag-env\\Lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
                        "Requirement already satisfied: jinja2 in .\\rag-env\\Lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
                        "Requirement already satisfied: setuptools in .\\rag-env\\Lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.10.2)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in .\\rag-env\\Lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
                        "Requirement already satisfied: colorama in .\\rag-env\\Lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in .\\rag-env\\Lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
                        "Requirement already satisfied: joblib>=1.3.0 in .\\rag-env\\Lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
                        "Requirement already satisfied: threadpoolctl>=3.2.0 in .\\rag-env\\Lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
                        "Requirement already satisfied: click>=8.0.0 in .\\rag-env\\Lib\\site-packages (from typer-slim->transformers<6.0.0,>=4.41.0->sentence-transformers) (8.3.1)\n"
                    ]
                }
            ],
            "source": [
                "!pip install qdrant-client langchain langchain-google-genai \\\n",
                "             langchain-text-splitters \\\n",
                "             sentence-transformers \\\n",
                "             python-dotenv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "id": "5b65ad10",
            "metadata": {},
            "outputs": [],
            "source": [
                "response_cache = {}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "cell_load_pdf",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Rag\\rag-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:25: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
                        "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
                        "c:\\Rag\\rag-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 11 pages\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'language': 'en-US', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'type': 'Conference Proceedings', 'firstpage': '5998', 'lastpage': '6008', 'source': 'Transformers.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.')"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "\n",
                "loader = PyPDFLoader(\"Transformers.pdf\")\n",
                "data = loader.load()\n",
                "print(f\"Loaded {len(data)} pages\")\n",
                "data[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c3d4e5f6",
            "metadata": {},
            "source": [
                "## Step 2: Split Documents into Chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "cell_split",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total number of chunks: 43\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'language': 'en-US', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'type': 'Conference Proceedings', 'firstpage': '5998', 'lastpage': '6008', 'source': 'Transformers.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly')"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "docs = text_splitter.split_documents(data)\n",
                "\n",
                "print(f\"Total number of chunks: {len(docs)}\")\n",
                "docs[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d4e5f6g7",
            "metadata": {},
            "source": [
                "## Step 3: Vector Database connection using Qdrant Cloud\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "id": "05863358",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Connected to Qdrant Cloud\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from qdrant_client import QdrantClient\n",
                "from qdrant_client.models import VectorParams, Distance\n",
                "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "qdrant = QdrantClient(\n",
                "    url=os.getenv(\"QDRANT_URL\"),\n",
                "    api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
                "    timeout = 60 \n",
                ")\n",
                "\n",
                "print(\"Connected to Qdrant Cloud\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "548ebf95",
            "metadata": {},
            "source": [
                "## Adding the Embeddings from HuggingFace\n",
                " model idsentence-transformers/all-MiniLM-L6-v2\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\sraja\\AppData\\Local\\Temp\\ipykernel_23444\\292730654.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
                        "  embeddings = HuggingFaceEmbeddings(\n",
                        "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 917.90it/s, Materializing param=pooler.dense.weight]                             \n",
                        "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
                        "Key                     | Status     |  | \n",
                        "------------------------+------------+--+-\n",
                        "embeddings.position_ids | UNEXPECTED |  | \n",
                        "\n",
                        "Notes:\n",
                        "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
                        "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
                        "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Embedding dimension: 384\n"
                    ]
                }
            ],
            "source": [
                "embeddings = HuggingFaceEmbeddings(\n",
                "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
                ")\n",
                "\n",
                "vector_size = len(embeddings.embed_query(\"test\"))\n",
                "print(\"Embedding dimension:\", vector_size)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bc0c1379",
            "metadata": {},
            "source": [
                "## Adding BM25 on retrievd docs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "id": "2c2a5aef",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting rank_bm25\n",
                        "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
                        "Requirement already satisfied: numpy in .\\rag-env\\Lib\\site-packages (from rank_bm25) (2.3.5)\n",
                        "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
                        "Installing collected packages: rank_bm25\n",
                        "Successfully installed rank_bm25-0.2.2\n"
                    ]
                }
            ],
            "source": [
                "!pip install rank_bm25"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "id": "4c105f69",
            "metadata": {},
            "outputs": [],
            "source": [
                "from rank_bm25 import BM25Okapi\n",
                "\n",
                "async def hybrid_retrieve(query, selected):\n",
                "    tasks = [retrieve(c, query) for c in selected]\n",
                "    results = await asyncio.gather(*tasks)\n",
                "\n",
                "    merged = []\n",
                "    for r in results:\n",
                "        merged.extend(r)\n",
                "\n",
                "    # Step 1: Vector ranking\n",
                "    merged.sort(key=lambda x: x[1], reverse=True)\n",
                "    top_vector_docs = [doc for doc, score in merged[:6]]\n",
                "\n",
                "    # Step 2: BM25 reranking\n",
                "    tokenized_docs = [doc.split() for doc in top_vector_docs]\n",
                "    bm25 = BM25Okapi(tokenized_docs)\n",
                "\n",
                "    tokenized_query = query.split()\n",
                "    bm25_scores = bm25.get_scores(tokenized_query)\n",
                "\n",
                "    combined = list(zip(top_vector_docs, bm25_scores))\n",
                "    combined.sort(key=lambda x: x[1], reverse=True)\n",
                "\n",
                "    # Return top 5 after hybrid rerank\n",
                "    return [doc for doc, score in combined[:5]]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "96909b5d",
            "metadata": {},
            "source": [
                "## Creating the collections of 4 different systems \n",
                "1.reserach_papers: any research paper data will give based on them \n",
                "2.knowledge_base: any knowledge based business ans can give\n",
                "3.code_docs: any documenation github repo based api documentation\n",
                "4.faq_data:Question AND ANSWERS Combine ans can give"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "id": "1d7a054d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collections ready\n"
                    ]
                }
            ],
            "source": [
                "collections = [\n",
                "    \"research_papers\",\n",
                "    \"knowledge_base\",\n",
                "    \"code_docs\",\n",
                "    \"faq_data\"\n",
                "]\n",
                "\n",
                "for name in collections:\n",
                "    if name not in [c.name for c in qdrant.get_collections().collections]:\n",
                "        qdrant.create_collection(\n",
                "            collection_name=name,\n",
                "            vectors_config=VectorParams(\n",
                "                size=vector_size,\n",
                "                distance=Distance.COSINE\n",
                "            )\n",
                "        )\n",
                "\n",
                "print(\"Collections ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "64d8ba84",
            "metadata": {},
            "source": [
                "## Now Adding the Ingestion Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "8f912f38",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Research pipeline\n",
                "\n",
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "import uuid\n",
                "\n",
                "def ingest_research(pdf_path):\n",
                "    loader = PyPDFLoader(pdf_path)\n",
                "    docs = loader.load()\n",
                "\n",
                "    splitter = RecursiveCharacterTextSplitter(\n",
                "        chunk_size=1000,\n",
                "        chunk_overlap=200\n",
                "    )\n",
                "\n",
                "    chunks = splitter.split_documents(docs)\n",
                "\n",
                "    vectors = embeddings.embed_documents(\n",
                "        [doc.page_content for doc in chunks]\n",
                "    )\n",
                "\n",
                "    points = [\n",
                "        {\n",
                "            \"id\": str(uuid.uuid4()),\n",
                "            \"vector\": vectors[i],\n",
                "            \"payload\": {\n",
                "                \"text\": chunks[i].page_content,\n",
                "                \"page\": chunks[i].metadata.get(\"page\"),\n",
                "                \"source_file\": pdf_path,\n",
                "                \"collection\": \"research_papers\"\n",
                "            }\n",
                "        }\n",
                "        for i in range(len(chunks))\n",
                "    ]\n",
                "\n",
                "    qdrant.upsert(\n",
                "        collection_name=\"research_papers\",\n",
                "        points=points\n",
                "    )\n",
                "\n",
                "    print(f\"Research ingested: {len(points)} chunks\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "dfd44959",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Knowledge Base Ingestion Pipeline\n",
                "\n",
                "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
                "\n",
                "def ingest_knowledge_base(folder_path):\n",
                "    loader = DirectoryLoader(\n",
                "        folder_path,\n",
                "        glob=\"**/*.md\",\n",
                "        loader_cls=TextLoader\n",
                "    )\n",
                "\n",
                "    docs = loader.load()\n",
                "\n",
                "    splitter = RecursiveCharacterTextSplitter(\n",
                "        chunk_size=800,\n",
                "        chunk_overlap=100\n",
                "    )\n",
                "\n",
                "    chunks = splitter.split_documents(docs)\n",
                "\n",
                "    vectors = embeddings.embed_documents(\n",
                "        [doc.page_content for doc in chunks]\n",
                "    )\n",
                "\n",
                "    points = [\n",
                "        {\n",
                "            \"id\": str(uuid.uuid4()),\n",
                "            \"vector\": vectors[i],\n",
                "            \"payload\": {\n",
                "                \"text\": chunks[i].page_content,\n",
                "                \"source_file\": chunks[i].metadata.get(\"source\"),\n",
                "                \"collection\": \"knowledge_base\"\n",
                "            }\n",
                "        }\n",
                "        for i in range(len(chunks))\n",
                "    ]\n",
                "\n",
                "    qdrant.upsert(\n",
                "        collection_name=\"knowledge_base\",\n",
                "        points=points\n",
                "    )\n",
                "\n",
                "    print(f\"Knowledge base ingested: {len(points)} chunks\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "06032047",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Code Docs Ingestion Pipeline\n",
                "\n",
                "def ingest_code_docs(repo_path):\n",
                "    loader = DirectoryLoader(\n",
                "        repo_path,\n",
                "        glob=\"**/*.py\",\n",
                "        loader_cls=TextLoader\n",
                "    )\n",
                "\n",
                "    docs = loader.load()\n",
                "\n",
                "    splitter = RecursiveCharacterTextSplitter(\n",
                "        chunk_size=500,\n",
                "        chunk_overlap=50\n",
                "    )\n",
                "\n",
                "    chunks = splitter.split_documents(docs)\n",
                "\n",
                "    vectors = embeddings.embed_documents(\n",
                "        [doc.page_content for doc in chunks]\n",
                "    )\n",
                "\n",
                "    points = [\n",
                "        {\n",
                "            \"id\": str(uuid.uuid4()),\n",
                "            \"vector\": vectors[i],\n",
                "            \"payload\": {\n",
                "                \"text\": chunks[i].page_content,\n",
                "                \"file\": chunks[i].metadata.get(\"source\"),\n",
                "                \"collection\": \"code_docs\"\n",
                "            }\n",
                "        }\n",
                "        for i in range(len(chunks))\n",
                "    ]\n",
                "\n",
                "    qdrant.upsert(\n",
                "        collection_name=\"code_docs\",\n",
                "        points=points\n",
                "    )\n",
                "\n",
                "    print(f\"Code docs ingested: {len(points)} chunks\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "bc44a8a2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# FAQ Data Ingestion Pipeline\n",
                "def ingest_faq(csv_path):\n",
                "    df = pd.read_csv(csv_path)\n",
                "\n",
                "    documents = []\n",
                "\n",
                "    for _, row in df.iterrows():\n",
                "        content = f\"Question: {row['question']}\\nAnswer: {row['answer']}\"\n",
                "\n",
                "        documents.append(\n",
                "            {\n",
                "                \"id\": str(uuid.uuid4()),\n",
                "                \"text\": content,\n",
                "                \"category\": row.get(\"category\", \"general\")\n",
                "            }\n",
                "        )\n",
                "\n",
                "    vectors = embeddings.embed_documents(\n",
                "        [doc[\"text\"] for doc in documents]\n",
                "    )\n",
                "\n",
                "    points = [\n",
                "        {\n",
                "            \"id\": documents[i][\"id\"],\n",
                "            \"vector\": vectors[i],\n",
                "            \"payload\": {\n",
                "                \"text\": documents[i][\"text\"],\n",
                "                \"category\": documents[i][\"category\"],\n",
                "                \"collection\": \"faq_data\"\n",
                "            }\n",
                "        }\n",
                "        for i in range(len(documents))\n",
                "    ]\n",
                "\n",
                "    qdrant.upsert(\n",
                "        collection_name=\"faq_data\",\n",
                "        points=points\n",
                "    )\n",
                "\n",
                "    print(f\"FAQ data ingested: {len(points)} entries\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "880e63ae",
            "metadata": {},
            "outputs": [],
            "source": [
                "#MASTER INGEST FUNCTION\n",
                "\n",
                "def ingest_all():\n",
                "    ingest_research(\"Transformers.pdf\")\n",
                "    ingest_knowledge_base(\"./knowledge_docs/\")\n",
                "    ingest_code_docs(\"./repo/\")\n",
                "    ingest_faq(\"faq.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "f6g7h8i9",
            "metadata": {},
            "source": [
                "## Step 5: Set Up Gemini 2.5 Flash & Ask Questions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "cell_llm",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Gemini 2.5 Flash LLM ready!\n"
                    ]
                }
            ],
            "source": [
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3, max_tokens=500)\n",
                "print(\"Gemini 2.5 Flash LLM ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "id": "f13e3302",
            "metadata": {},
            "outputs": [],
            "source": [
                "def stream_answer(prompt):\n",
                "    response = llm.stream(prompt)\n",
                "\n",
                "    full_answer = \"\"\n",
                "    for chunk in response:\n",
                "        print(chunk.content, end=\"\", flush=True)\n",
                "        full_answer += chunk.content\n",
                "\n",
                "    print()  # newline\n",
                "    return full_answer"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4d475f88",
            "metadata": {},
            "source": [
                "# Rewrite query "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "4872fcdd",
            "metadata": {},
            "outputs": [],
            "source": [
                "def rewrite_query(question):\n",
                "    rewrite_prompt = f\"\"\"\n",
                "    Rewrite the following question into ONE clear standalone search query.\n",
                "\n",
                "    Return ONLY the rewritten query.\n",
                "    Do NOT explain.\n",
                "    Do NOT give options.\n",
                "\n",
                "    Question: {question}\n",
                "    \"\"\"\n",
                "\n",
                "    response = llm.invoke(rewrite_prompt)\n",
                "    return response.content.strip()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "42e52b74",
            "metadata": {},
            "source": [
                "## Context Builder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "cell_ask_fn",
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_context_with_sources(docs):\n",
                "    context = \"\"\n",
                "    sources = []\n",
                "\n",
                "    for doc in docs:\n",
                "        page = doc.metadata.get(\"page\", \"unknown\")\n",
                "        context += f\"[Page {page}]\\n{doc.page_content}\\n\\n\"\n",
                "        sources.append(page)\n",
                "\n",
                "    return context, list(set(sources))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3601b11a",
            "metadata": {},
            "source": [
                "# Adding the Memory For RAG "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "id": "af204c93",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1.2.10\n"
                    ]
                }
            ],
            "source": [
                "import langchain\n",
                "print(langchain.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "2e1c9148",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple memory storage\n",
                "chat_history = []\n",
                "\n",
                "def update_memory(user_input, assistant_output):\n",
                "    chat_history.append({\n",
                "        \"user\": user_input,\n",
                "        \"assistant\": assistant_output\n",
                "    })\n",
                "\n",
                "    # Optional: keep only last 5 conversations\n",
                "    if len(chat_history) > 5:\n",
                "        chat_history.pop(0)\n",
                "\n",
                "\n",
                "def format_chat_history():\n",
                "    formatted = \"\"\n",
                "    for turn in chat_history:\n",
                "        formatted += f\"User: {turn['user']}\\n\"\n",
                "        formatted += f\"Assistant: {turn['assistant']}\\n\\n\"\n",
                "    return formatted"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "5e20eba4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Adding the Hybrid logic \n",
                "\n",
                "COLLECTION_CONFIDENCE = {\n",
                "    \"research_papers\": 1.0,\n",
                "    \"knowledge_base\": 0.8,\n",
                "    \"code_docs\": 1.2,\n",
                "    \"faq_data\": 0.7\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "1d17220e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# adding the Dynamic K\n",
                "def dynamic_k(name):\n",
                "    if name == \"code_docs\":\n",
                "        return 5\n",
                "    return 3\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "dff95367",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hybrid Planner\n",
                "\n",
                "def planner(question):\n",
                "    if \"api\" in question or \"function\" in question:\n",
                "        return [\"code_docs\", \"research_papers\"]\n",
                "    if \"how\" in question:\n",
                "        return [\"faq_data\", \"knowledge_base\"]\n",
                "    return [\"research_papers\", \"knowledge_base\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "id": "45205a55",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: nest_asyncio in .\\rag-env\\Lib\\site-packages (1.6.0)\n"
                    ]
                }
            ],
            "source": [
                "!pip install nest_asyncio"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "62a81371",
            "metadata": {},
            "outputs": [],
            "source": [
                "import nest_asyncio\n",
                "nest_asyncio.apply()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "id": "938477bb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Async Hybrid Retrieval\n",
                "\n",
                "import asyncio\n",
                "\n",
                "async def retrieve(collection, query, filter_condition=None):\n",
                "    vector = embeddings.embed_query(query)\n",
                "    k = dynamic_k(collection)\n",
                "\n",
                "    results = qdrant.query_points(\n",
                "        collection_name=collection,\n",
                "        query=vector,\n",
                "        limit=k,\n",
                "        query_filter=filter_condition\n",
                "    )\n",
                "\n",
                "    return [\n",
                "        (\n",
                "            point.payload[\"text\"],\n",
                "            point.score * COLLECTION_CONFIDENCE[collection]\n",
                "        )\n",
                "        for point in results.points\n",
                "    ]\n",
                "\n",
                "\n",
                "async def hybrid_retrieve(query, selected):\n",
                "    tasks = [retrieve(c, query) for c in selected]\n",
                "    results = await asyncio.gather(*tasks)\n",
                "\n",
                "    merged = []\n",
                "    for r in results:\n",
                "        merged.extend(r)\n",
                "\n",
                "    merged.sort(key=lambda x: x[1], reverse=True)\n",
                "\n",
                "    return [doc for doc, score in merged[:6]]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "g7h8i9j0",
            "metadata": {},
            "source": [
                "## Convert to agent "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "e78aa8ab",
            "metadata": {},
            "outputs": [],
            "source": [
                "def document_search(query):\n",
                "    docs = retriever.invoke(query)\n",
                "    return docs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "id": "a5fad4a3",
            "metadata": {},
            "outputs": [],
            "source": [
                "async def hybrid_agent(question):\n",
                "\n",
                "    if question in response_cache:\n",
                "        print(\" Cached response\")\n",
                "        return response_cache[question]\n",
                "\n",
                "    rewritten = rewrite_query(question)\n",
                "    selected = planner(rewritten)\n",
                "\n",
                "    docs = await hybrid_retrieve(rewritten, selected)\n",
                "\n",
                "    context = \"\\n\\n\".join(docs)\n",
                "    memory = format_chat_history()\n",
                "\n",
                "    prompt = f\"\"\"\n",
                "    You are a multi-resource AI assistant.\n",
                "\n",
                "    Previous Conversation:\n",
                "    {memory}\n",
                "\n",
                "    Context:\n",
                "    {context}\n",
                "\n",
                "    Question:\n",
                "    {question}\n",
                "    \"\"\"\n",
                "\n",
                "    answer = stream_answer(prompt)\n",
                "\n",
                "    update_memory(question, answer)\n",
                "\n",
                "    response_cache[question] = answer\n",
                "\n",
                "    return answer"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0beb13f5",
            "metadata": {},
            "source": [
                "## Adding the Evaluation Cell"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "id": "77b8ddcc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "range dependencies and contextual relationships between words, regardless of their position in the sequence.\n",
                        "\n",
                        "At its\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'range dependencies and contextual relationships between words, regardless of their position in the sequence.\\n\\nAt its'"
                        ]
                    },
                    "execution_count": 57,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "await hybrid_agent(\"Explain how attention works in the transformer model.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "rag-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
