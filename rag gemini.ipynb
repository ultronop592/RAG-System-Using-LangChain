{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a1b2c3d4",
            "metadata": {},
            "source": [
                "# RAG Q&A with Google Gemini 2.5 Flash\n",
                "\n",
                "This notebook demonstrates a Retrieval-Augmented Generation (RAG) pipeline:\n",
                "1. Load a PDF document\n",
                "2. Split into chunks\n",
                "3. Create embeddings & vector store (FAISS)\n",
                "4. Ask questions using Gemini 2.5 Flash"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b2c3d4e5",
            "metadata": {},
            "source": [
                "## Step 1: Load PDF Document"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_load_pdf",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "\n",
                "loader = PyPDFLoader(\"Transformers.pdf\")\n",
                "data = loader.load()\n",
                "print(f\"Loaded {len(data)} pages\")\n",
                "data[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c3d4e5f6",
            "metadata": {},
            "source": [
                "## Step 2: Split Documents into Chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_split",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "docs = text_splitter.split_documents(data)\n",
                "\n",
                "print(f\"Total number of chunks: {len(docs)}\")\n",
                "docs[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d4e5f6g7",
            "metadata": {},
            "source": [
                "## Step 3: Create Embeddings & Vector Store\n",
                "\n",
                "Using HuggingFace `all-MiniLM-L6-v2` embeddings with FAISS vector store."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_embeddings",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
                "from langchain_community.vectorstores import FAISS\n",
                "\n",
                "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "\n",
                "# Test embedding\n",
                "vector = embeddings.embed_query(\"hello, world!\")\n",
                "print(f\"Embedding dimension: {len(vector)}\")\n",
                "vector[:5]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_vectorstore",
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorstore = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
                "print(\"Vector store created successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e5f6g7h8",
            "metadata": {},
            "source": [
                "## Step 4: Test Retrieval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_retriever",
            "metadata": {},
            "outputs": [],
            "source": [
                "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
                "\n",
                "retrieved_docs = retriever.invoke(\"What is the main topic of this paper?\")\n",
                "print(f\"Retrieved {len(retrieved_docs)} chunks\")\n",
                "\n",
                "# Preview first 3 chunks\n",
                "for i, doc in enumerate(retrieved_docs[:3]):\n",
                "    print(f\"\\n--- Chunk {i+1} (Page {doc.metadata.get('page', '?')}) ---\")\n",
                "    print(doc.page_content[:200] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f6g7h8i9",
            "metadata": {},
            "source": [
                "## Step 5: Set Up Gemini 2.5 Flash & Ask Questions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_llm",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3, max_tokens=500)\n",
                "print(\"Gemini 2.5 Flash LLM ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_ask_fn",
            "metadata": {},
            "outputs": [],
            "source": [
                "def ask_question(question):\n",
                "    \"\"\"Ask a question using RAG pipeline\"\"\"\n",
                "    # Retrieve relevant context\n",
                "    docs = retriever.invoke(question)\n",
                "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
                "    \n",
                "    # Build prompt\n",
                "    prompt = f\"\"\"You are an assistant for question-answering tasks.\n",
                "Use the following pieces of retrieved context to answer the question.\n",
                "If you don't know the answer, say that you don't know.\n",
                "Use three sentences maximum and keep the answer concise.\n",
                "\n",
                "Context: {context}\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\"\n",
                "    \n",
                "    response = llm.invoke(prompt)\n",
                "    return response.content\n",
                "\n",
                "print(\"ask_question() function ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "g7h8i9j0",
            "metadata": {},
            "source": [
                "## Step 6: Ask Questions!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_q1",
            "metadata": {},
            "outputs": [],
            "source": [
                "answer = ask_question(\"What is the main contribution of this paper?\")\n",
                "print(answer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_q2",
            "metadata": {},
            "outputs": [],
            "source": [
                "answer = ask_question(\"What methods or models are discussed in this paper?\")\n",
                "print(answer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_q3",
            "metadata": {},
            "outputs": [],
            "source": [
                "answer = ask_question(\"What are the key results or findings?\")\n",
                "print(answer)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
