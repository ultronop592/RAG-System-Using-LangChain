{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a1b2c3d4",
            "metadata": {},
            "source": [
                "# RAG Q&A with Google Gemini 2.5 Flash\n",
                "\n",
                "This notebook demonstrates a Retrieval-Augmented Generation (RAG) pipeline:\n",
                "1. Load a PDF document\n",
                "2. Split into chunks\n",
                "3. Create embeddings & vector store (FAISS)\n",
                "4. Ask questions using Gemini 2.5 Flash"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b2c3d4e5",
            "metadata": {},
            "source": [
                "## Step 1: Load PDF Document"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "id": "cell_load_pdf",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 11 pages\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'language': 'en-US', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'type': 'Conference Proceedings', 'firstpage': '5998', 'lastpage': '6008', 'source': 'Transformers.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.')"
                        ]
                    },
                    "execution_count": 34,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "\n",
                "loader = PyPDFLoader(\"Transformers.pdf\")\n",
                "data = loader.load()\n",
                "print(f\"Loaded {len(data)} pages\")\n",
                "data[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c3d4e5f6",
            "metadata": {},
            "source": [
                "## Step 2: Split Documents into Chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "id": "cell_split",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total number of chunks: 43\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'language': 'en-US', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'type': 'Conference Proceedings', 'firstpage': '5998', 'lastpage': '6008', 'source': 'Transformers.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly')"
                        ]
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "docs = text_splitter.split_documents(data)\n",
                "\n",
                "print(f\"Total number of chunks: {len(docs)}\")\n",
                "docs[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d4e5f6g7",
            "metadata": {},
            "source": [
                "## Step 3: Create Embeddings & Vector Store\n",
                "\n",
                "Using HuggingFace `all-MiniLM-L6-v2` embeddings with FAISS vector store."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "cell_embeddings",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1038.30it/s, Materializing param=pooler.dense.weight]                             \n",
                        "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
                        "Key                     | Status     |  | \n",
                        "------------------------+------------+--+-\n",
                        "embeddings.position_ids | UNEXPECTED |  | \n",
                        "\n",
                        "Notes:\n",
                        "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Embedding dimension: 384\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[-0.038177136331796646,\n",
                            " 0.03291105851531029,\n",
                            " -0.00545938266441226,\n",
                            " 0.014369907788932323,\n",
                            " -0.040291059762239456]"
                        ]
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
                "from langchain_community.vectorstores import FAISS\n",
                "\n",
                "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "\n",
                "# Test embedding\n",
                "vector = embeddings.embed_query(\"hello, world!\")\n",
                "print(f\"Embedding dimension: {len(vector)}\")\n",
                "vector[:5]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "cell_vectorstore",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Vector store created successfully!\n"
                    ]
                }
            ],
            "source": [
                "vectorstore = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
                "print(\"Vector store created successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e5f6g7h8",
            "metadata": {},
            "source": [
                "## Step 4: Test Retrieval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "id": "cell_retriever",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Retrieved 5 chunks\n",
                        "\n",
                        "--- Chunk 1 (Page 8) ---\n",
                        "tensorflow/tensor2tensor.\n",
                        "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
                        "comments, corrections and inspiration.\n",
                        "9...\n",
                        "\n",
                        "--- Chunk 2 (Page 9) ---\n",
                        "2017.\n",
                        "[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
                        "In International Conference on Learning Representations , 2017.\n",
                        "[17] Diederik Kingma and Jimmy Ba. ...\n",
                        "\n",
                        "--- Chunk 3 (Page 0) ---\n",
                        "efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
                        "implementing tensor2tensor, replacing our earlier codebase, greatly improving results a...\n"
                    ]
                }
            ],
            "source": [
                "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
                "\n",
                "retrieved_docs = retriever.invoke(\"What is the main topic of this paper?\")\n",
                "print(f\"Retrieved {len(retrieved_docs)} chunks\")\n",
                "\n",
                "# Preview first 3 chunks\n",
                "for i, doc in enumerate(retrieved_docs[:3]):\n",
                "    print(f\"\\n--- Chunk {i+1} (Page {doc.metadata.get('page', '?')}) ---\")\n",
                "    print(doc.page_content[:200] + \"...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "id": "148c446c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# adding the similarity score debug\n",
                "\n",
                "def debug_retrieval(query):\n",
                "    results = vectorstore.similarity_search_with_score(query, k=5)\n",
                "\n",
                "    for doc, score in results:\n",
                "        print(\"Score:\", round(score, 4))\n",
                "        print(\"Page:\", doc.metadata.get(\"page\"))\n",
                "        print(doc.page_content[:150])\n",
                "        print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "id": "0ec51134",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Adding the Re-ranking\n",
                "\n",
                "def rerank_docs(question, docs):\n",
                "    scored_docs = []\n",
                "\n",
                "    for doc in docs:\n",
                "        score_prompt = f\"\"\"\n",
                "        Rate from 1-10 how relevant this passage is \n",
                "        for answering the question.\n",
                "\n",
                "        Question: {question}\n",
                "\n",
                "        Passage:\n",
                "        {doc.page_content}\n",
                "\n",
                "        Only return a number.\n",
                "        \"\"\"\n",
                "\n",
                "        score = llm.invoke(score_prompt).content.strip()\n",
                "\n",
                "        try:\n",
                "            score = int(score)\n",
                "        except:\n",
                "            score = 0\n",
                "\n",
                "        scored_docs.append((doc, score))\n",
                "\n",
                "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
                "\n",
                "    return [doc for doc, _ in scored_docs[:3]]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f6g7h8i9",
            "metadata": {},
            "source": [
                "## Step 5: Set Up Gemini 2.5 Flash & Ask Questions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "id": "cell_llm",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Gemini 2.5 Flash LLM ready!\n"
                    ]
                }
            ],
            "source": [
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3, max_tokens=500)\n",
                "print(\"Gemini 2.5 Flash LLM ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4d475f88",
            "metadata": {},
            "source": [
                "# Rewrite query "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "id": "4872fcdd",
            "metadata": {},
            "outputs": [],
            "source": [
                "def rewrite_query(question):\n",
                "    rewrite_prompt = f\"\"\"\n",
                "    Rewrite the following question into ONE clear standalone search query.\n",
                "\n",
                "    Return ONLY the rewritten query.\n",
                "    Do NOT explain.\n",
                "    Do NOT give options.\n",
                "\n",
                "    Question: {question}\n",
                "    \"\"\"\n",
                "\n",
                "    response = llm.invoke(rewrite_prompt)\n",
                "    return response.content.strip()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "42e52b74",
            "metadata": {},
            "source": [
                "## Context Builder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "id": "cell_ask_fn",
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_context_with_sources(docs):\n",
                "    context = \"\"\n",
                "    sources = []\n",
                "\n",
                "    for doc in docs:\n",
                "        page = doc.metadata.get(\"page\", \"unknown\")\n",
                "        context += f\"[Page {page}]\\n{doc.page_content}\\n\\n\"\n",
                "        sources.append(page)\n",
                "\n",
                "    return context, list(set(sources))\n",
                "\n",
                "\n",
                "def ask_question(question):\n",
                "\n",
                "    # 1️ Retrieve relevant docs\n",
                "    docs = retriever.invoke(question)\n",
                "\n",
                "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
                "\n",
                "    # 2️ Get formatted memory\n",
                "    memory_text = format_chat_history()\n",
                "\n",
                "    # 3️ Build prompt with memory\n",
                "    prompt = f\"\"\"\n",
                "    You are an AI assistant answering questions from a document.\n",
                "\n",
                "    Previous Conversation:\n",
                "    {memory_text}\n",
                "\n",
                "    Use ONLY the following context to answer.\n",
                "    If answer is not found, say 'Not found in document.'\n",
                "\n",
                "    Context:\n",
                "    {context}\n",
                "\n",
                "    Question:\n",
                "    {question}\n",
                "    \"\"\"\n",
                "\n",
                "    # 4️ Generate response\n",
                "    response = llm.invoke(prompt)\n",
                "    answer = response.content\n",
                "\n",
                "    # 5️ Save memory\n",
                "    update_memory(question, answer)\n",
                "\n",
                "    return answer"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3601b11a",
            "metadata": {},
            "source": [
                "# Adding the Memory For RAG "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "id": "af204c93",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1.2.10\n"
                    ]
                }
            ],
            "source": [
                "import langchain\n",
                "print(langchain.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "id": "2e1c9148",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple memory storage\n",
                "chat_history = []\n",
                "\n",
                "def update_memory(user_input, assistant_output):\n",
                "    chat_history.append({\n",
                "        \"user\": user_input,\n",
                "        \"assistant\": assistant_output\n",
                "    })\n",
                "\n",
                "    # Optional: keep only last 5 conversations\n",
                "    if len(chat_history) > 5:\n",
                "        chat_history.pop(0)\n",
                "\n",
                "\n",
                "def format_chat_history():\n",
                "    formatted = \"\"\n",
                "    for turn in chat_history:\n",
                "        formatted += f\"User: {turn['user']}\\n\"\n",
                "        formatted += f\"Assistant: {turn['assistant']}\\n\\n\"\n",
                "    return formatted"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "g7h8i9j0",
            "metadata": {},
            "source": [
                "## Convert to agent "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "id": "e78aa8ab",
            "metadata": {},
            "outputs": [],
            "source": [
                "def document_search(query):\n",
                "    docs = retriever.invoke(query)\n",
                "    return docs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "id": "7d02f525",
            "metadata": {},
            "outputs": [],
            "source": [
                "def agentic_rag(question):\n",
                "\n",
                "    print(\" Step 1: Rewrite Query\")\n",
                "    rewritten_query = rewrite_query(question)\n",
                "    print(\"Rewritten:\", rewritten_query)\n",
                "\n",
                "    print(\" Step 2: Retrieve Documents\")\n",
                "    docs = retriever.invoke(rewritten_query)\n",
                "\n",
                "    print(\" Step 3: Re-rank Documents\")\n",
                "    docs = rerank_docs(question, docs)\n",
                "\n",
                "    print(\" Step 4: Build Context\")\n",
                "    context, sources = build_context_with_sources(docs)\n",
                "\n",
                "    memory_text = format_chat_history()\n",
                "\n",
                "    prompt = f\"\"\"\n",
                "    You are an AI assistant answering questions from a research paper.\n",
                "\n",
                "    Previous Conversation:\n",
                "    {memory_text}\n",
                "\n",
                "    Use ONLY the context below to answer.\n",
                "    Cite pages if relevant.\n",
                "    If answer not found, say \"Not found in document.\"\n",
                "\n",
                "    Context:\n",
                "    {context}\n",
                "\n",
                "    Question:\n",
                "    {question}\n",
                "\n",
                "    Answer clearly in 2-4 sentences.\n",
                "    \"\"\"\n",
                "\n",
                "    print(\" Step 5: Generate Answer\")\n",
                "    response = llm.invoke(prompt)\n",
                "    answer = response.content\n",
                "\n",
                "    update_memory(question, answer)\n",
                "\n",
                "    return answer, sources"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0beb13f5",
            "metadata": {},
            "source": [
                "## Adding the Evaluation Cell"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "id": "acc3c13a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "================================================================================\n",
                        "Question: What is Transformer?\n",
                        " Step 1: Rewrite Query\n",
                        "Rewritten: What is Transformer AI\n",
                        " Step 2: Retrieve Documents\n",
                        " Step 3: Re-rank Documents\n",
                        " Step 4: Build Context\n",
                        " Step 5: Generate Answer\n",
                        "Answer: The Transformer is a neural sequence transduction model that follows an encoder-decoder architecture. It utilizes stacked self-attention and point-wise, fully connected layers for both its encoder and decoder components. Notably, it is the first transduction model to rely entirely on self-attention to compute representations of its input and output, without employing sequence-aligned RNNs or convolution. (Page 1)\n",
                        "Sources: [1, 2]\n",
                        "================================================================================\n",
                        "Question: How is it different from RNN?\n",
                        " Step 1: Rewrite Query\n",
                        "Rewritten: LSTM vs RNN differences\n",
                        " Step 2: Retrieve Documents\n",
                        " Step 3: Re-rank Documents\n",
                        " Step 4: Build Context\n",
                        " Step 5: Generate Answer\n",
                        "Answer: The Transformer relies entirely on self-attention to compute representations of its input and output, without employing sequence-aligned RNNs or convolution (Page 1). A self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations (Page 5). This difference in operation also means self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality (Page 5).\n",
                        "Sources: [0, 1, 5]\n",
                        "================================================================================\n",
                        "Question: What is the main contribution?\n",
                        " Step 1: Rewrite Query\n",
                        "Rewritten: main contribution definition*\n",
                        " Step 2: Retrieve Documents\n",
                        " Step 3: Re-rank Documents\n"
                    ]
                },
                {
                    "ename": "ChatGoogleGenerativeAIError",
                    "evalue": "Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 31.896004035s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '31s'}]}}",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:3047\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3046\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3049\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\google\\genai\\models.py:5227\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5226\u001b[39m i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5227\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5229\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5231\u001b[39m function_map = _extra_utils.get_function_map(parsed_config)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\google\\genai\\models.py:4009\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4007\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4009\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4010\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4011\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4014\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4015\u001b[39m ):\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\google\\genai\\_api_client.py:1386\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1383\u001b[39m http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1384\u001b[39m     http_method, path, request_dict, http_options\n\u001b[32m   1385\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m response_body = (\n\u001b[32m   1388\u001b[39m     response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1389\u001b[39m )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\google\\genai\\_api_client.py:1220\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1219\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python314\\Lib\\concurrent\\futures\\_base.py:443\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python314\\Lib\\concurrent\\futures\\_base.py:395\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\google\\genai\\_api_client.py:1199\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1192\u001b[39m response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1193\u001b[39m     method=http_request.method,\n\u001b[32m   1194\u001b[39m     url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1197\u001b[39m     timeout=http_request.timeout,\n\u001b[32m   1198\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1199\u001b[39m \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1201\u001b[39m     response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1202\u001b[39m )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\google\\genai\\errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\google\\genai\\errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n",
                        "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 31.896004035s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '31s'}]}}",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQuestion:\u001b[39m\u001b[33m\"\u001b[39m, q)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m answer, sources = \u001b[43magentic_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAnswer:\u001b[39m\u001b[33m\"\u001b[39m, answer)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSources:\u001b[39m\u001b[33m\"\u001b[39m, sources)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36magentic_rag\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m      8\u001b[39m docs = retriever.invoke(rewritten_query)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Step 3: Re-rank Documents\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m docs = \u001b[43mrerank_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Step 4: Build Context\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m context, sources = build_context_with_sources(docs)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mrerank_docs\u001b[39m\u001b[34m(question, docs)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[32m      7\u001b[39m     score_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m    Rate from 1-10 how relevant this passage is \u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m    for answering the question.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[33m    Only return a number.\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     score = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_prompt\u001b[49m\u001b[43m)\u001b[49m.content.strip()\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     22\u001b[39m         score = \u001b[38;5;28mint\u001b[39m(score)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2535\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   2532\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2533\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m2535\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1123\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1114\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1116\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1120\u001b[39m     **kwargs: Any,\n\u001b[32m   1121\u001b[39m ) -> LLMResult:\n\u001b[32m   1122\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:933\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    932\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    939\u001b[39m         )\n\u001b[32m    940\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    941\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1235\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1233\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1234\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1235\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1239\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:3051\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28mself\u001b[39m.client.models.generate_content(\n\u001b[32m   3048\u001b[39m         **request,\n\u001b[32m   3049\u001b[39m     )\n\u001b[32m   3050\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m3051\u001b[39m     \u001b[43m_handle_client_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Rag\\rag-env\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:145\u001b[39m, in \u001b[36m_handle_client_error\u001b[39m\u001b[34m(e, request)\u001b[39m\n\u001b[32m    143\u001b[39m model_name = request.get(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError calling model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
                        "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 31.896004035s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '31s'}]}}"
                    ]
                }
            ],
            "source": [
                "test_questions = [\n",
                "    \"What is Transformer?\",\n",
                "    \"How is it different from RNN?\",\n",
                "    \"What is the main contribution?\"\n",
                "]\n",
                "\n",
                "for q in test_questions:\n",
                "    print(\"=\"*80)\n",
                "    print(\"Question:\", q)\n",
                "    answer, sources = agentic_rag(q)\n",
                "    print(\"Answer:\", answer)\n",
                "    print(\"Sources:\", sources)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "rag-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
